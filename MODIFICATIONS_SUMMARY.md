# ä¿®æ”¹ç¸½çµ / Modifications Summary

## æ¦‚è¿° / Overview

æœ¬æ¬¡æ“´å±•ç‚º Context Rot å¯¦é©—æ¡†æ¶æ–°å¢äº† Ollama æ”¯æ´ï¼Œä½¿å…¶èƒ½å¤ ä½¿ç”¨æœ¬åœ°é–‹æºæ¨¡å‹é€²è¡Œé›¢ç·šå¯¦é©—å’Œè©•ä¼°ã€‚

This extension adds Ollama support to the Context Rot experiment framework, enabling offline experimentation and evaluation with local open-source models.

---

## æª”æ¡ˆä¿®æ”¹æ¸…å–® / File Modification List

### âœ¨ æ–°å¢æª”æ¡ˆ / New Files

#### æ ¸å¿ƒå¯¦ç¾ / Core Implementation

1. **`experiments/models/providers/ollama.py`**
   - Ollama provider å¯¦ç¾
   - æ”¯æ´æœ¬åœ°æ¨¡å‹æ¨ç†
   - å¯é…ç½®çš„ Ollama ä¸»æ©Ÿä½å€

2. **`experiments/models/providers/__init__.py`**
   - Provider æ¨¡çµ„åˆå§‹åŒ–
   - çµ±ä¸€å°å‡ºæ‰€æœ‰ provider é¡åˆ¥

#### æ–‡æª” / Documentation

3. **`CHANGELOG.md`**
   - è©³ç´°çš„è®Šæ›´æ—¥èªŒ
   - åˆ—å‡ºæ‰€æœ‰æ–°å¢åŠŸèƒ½å’Œç”¨é€”

4. **`OLLAMA_INTEGRATION.md`**
   - å®Œæ•´çš„è‹±æ–‡æ•´åˆæ–‡æª”
   - åŒ…å«æ¶æ§‹èªªæ˜ã€ä½¿ç”¨ç¯„ä¾‹ã€æ•ˆèƒ½è€ƒé‡

5. **`OLLAMA_INTEGRATION_zh-TW.md`**
   - å®Œæ•´çš„ä¸­æ–‡æ•´åˆæ–‡æª”
   - å¿«é€Ÿé–‹å§‹æŒ‡å—å’Œå¸¸è¦‹å•é¡Œ

6. **`MODIFICATIONS_SUMMARY.md`** (æœ¬æª”æ¡ˆ)
   - ä¿®æ”¹ç¸½çµ

#### ç¯„ä¾‹å’Œæ¸¬è©¦ / Examples and Tests

7. **`examples/ollama_example.py`**
   - Ollama ä½¿ç”¨ç¯„ä¾‹
   - å±•ç¤ºåŸºæœ¬ç”¨æ³•å’Œ judge é…ç½®

8. **`tests/test_ollama_provider.py`**
   - å®Œæ•´çš„æ¸¬è©¦å¥—ä»¶
   - æ¶µè“‹å®¢æˆ¶ç«¯åˆå§‹åŒ–ã€å–®ä¸€æç¤ºã€æ‰¹æ¬¡è™•ç†

### ğŸ“ ä¿®æ”¹æª”æ¡ˆ / Modified Files

1. **`experiments/models/llm_judge.py`**
   ```python
   # æ–°å¢å…§å®¹ / Added:
   - provider åƒæ•¸ï¼ˆé è¨­ç‚º "openai"ï¼‰
   - _get_provider() æ–¹æ³•ç”¨æ–¼å‹•æ…‹é¸æ“‡ provider
   - å°å…¥æ‰€æœ‰ provider é¡åˆ¥

   # è®Šæ›´æ‘˜è¦ / Changes:
   - ç¬¬ 1-9 è¡Œï¼šæ–°å¢ import
   - ç¬¬ 12 è¡Œï¼šæ–°å¢ provider åƒæ•¸
   - ç¬¬ 21-32 è¡Œï¼šæ–°å¢ _get_provider() æ–¹æ³•
   ```

2. **`requirements.txt`**
   ```diff
   + ollama>=0.4.8
   ```

3. **`experiments/models/README.md`**
   ```markdown
   # æ–°å¢å…§å®¹ / Added:
   - Ollama provider èªªæ˜
   - ç’°å¢ƒè®Šæ•¸é…ç½®
   - è¨­ç½®æ­¥é©Ÿ
   - ä½¿ç”¨ç¯„ä¾‹ï¼ˆæ¸¬è©¦å’Œè©•åˆ¤ï¼‰
   ```

4. **`README.md`**
   ```markdown
   # æ–°å¢å…§å®¹ / Added:
   - Ollama ç’°å¢ƒè®Šæ•¸èªªæ˜
   - å¯é¸çš„ Ollama å®‰è£æ­¥é©Ÿ
   - æ¨¡å‹æ‹‰å–æŒ‡ä»¤
   ```

---

## åŠŸèƒ½ç‰¹æ€§ / Features

### âœ… å·²å¯¦ç¾ / Implemented

- [x] Ollama provider åŸºç¤å¯¦ç¾
- [x] èˆ‡ç¾æœ‰æ¶æ§‹å®Œå…¨æ•´åˆ
- [x] LLMJudge æ”¯æ´å¯é…ç½®çš„ provider
- [x] å®Œæ•´çš„è‹±æ–‡å’Œä¸­æ–‡æ–‡æª”
- [x] ç¯„ä¾‹è…³æœ¬
- [x] æ¸¬è©¦å¥—ä»¶
- [x] å‘å¾Œç›¸å®¹æ€§

### ğŸ¯ ä¸»è¦ç”¨é€” / Key Use Cases

1. **é›¢ç·šå¯¦é©—** - ç„¡éœ€ç¶²è·¯é€£ç·š
2. **æˆæœ¬ç¯€çœ** - ç„¡ API è²»ç”¨
3. **éš±ç§ä¿è­·** - è³‡æ–™ä¿æŒæœ¬åœ°
4. **å­¸è¡“ç ”ç©¶** - å¯é‡ç¾çš„æœ¬åœ°æ¨¡å‹
5. **æ¨¡å‹æ¯”è¼ƒ** - API vs æœ¬åœ°æ¨¡å‹

---

## æŠ€è¡“ç´°ç¯€ / Technical Details

### æ¶æ§‹è¨­è¨ˆ / Architecture

```
BaseProvider (æŠ½è±¡åŸºé¡)
    â”œâ”€â”€ OpenAIProvider
    â”œâ”€â”€ AnthropicProvider
    â”œâ”€â”€ GoogleProvider
    â””â”€â”€ OllamaProvider (æ–°å¢)
```

### é—œéµæ–¹æ³• / Key Methods

**OllamaProvider:**
- `process_single_prompt()` - è™•ç†å–®ä¸€æç¤º
- `get_client()` - åˆå§‹åŒ– Ollama å®¢æˆ¶ç«¯

**LLMJudge:**
- `_get_provider()` - å‹•æ…‹é¸æ“‡ providerï¼ˆæ–°å¢ï¼‰

### ç’°å¢ƒè®Šæ•¸ / Environment Variables

| è®Šæ•¸ | é è¨­å€¼ | èªªæ˜ |
|------|--------|------|
| `OLLAMA_HOST` | `http://localhost:11434` | Ollama ä¼ºæœå™¨ä½å€ |

---

## ä½¿ç”¨ç¯„ä¾‹ / Usage Examples

### ç¯„ä¾‹ 1ï¼šä½¿ç”¨ Ollama é€²è¡Œæ¸¬è©¦

```python
from models.providers.ollama import OllamaProvider

provider = OllamaProvider()
provider.main(
    input_path="input.csv",
    output_path="output.csv",
    input_column="prompt",
    output_column="response",
    model_name="llama3.1:8b",
    max_context_length=128000,
    max_tokens_per_minute=1000000
)
```

### ç¯„ä¾‹ 2ï¼šä½¿ç”¨ Ollama ä½œç‚ºè©•åˆ¤å“¡

```python
from models.llm_judge import LLMJudge

judge = LLMJudge(
    prompt=judge_prompt,
    model_name="qwen2.5:14b",
    provider="ollama"
)

judge.evaluate(
    input_path="results.csv",
    output_path="judged.csv",
    max_context_length=128000,
    max_tokens_per_minute=1000000
)
```

---

## æ¸¬è©¦ / Testing

### é‹è¡Œæ¸¬è©¦ / Run Tests

```bash
# å®Œæ•´æ¸¬è©¦å¥—ä»¶
python tests/test_ollama_provider.py

# ç¯„ä¾‹è…³æœ¬
python examples/ollama_example.py
```

### æ¸¬è©¦è¦†è“‹ / Test Coverage

- âœ… å®¢æˆ¶ç«¯åˆå§‹åŒ–
- âœ… å–®ä¸€æç¤ºè™•ç†
- âœ… æ‰¹æ¬¡ CSV è™•ç†
- âœ… éŒ¯èª¤è™•ç†
- âœ… ç©ºå›æ‡‰æª¢æ¸¬

---

## ç›¸å®¹æ€§ / Compatibility

### å‘å¾Œç›¸å®¹ / Backward Compatibility

âœ… **å®Œå…¨å‘å¾Œç›¸å®¹**
- ç¾æœ‰ç¨‹å¼ç¢¼ç„¡éœ€ä¿®æ”¹
- LLMJudge é è¨­ä½¿ç”¨ OpenAI
- å…¶ä»– provider ä¸å—å½±éŸ¿

### Python ç‰ˆæœ¬ / Python Version

- Python >= 3.8

### ä¾è³´ç‰ˆæœ¬ / Dependencies

- `ollama >= 0.4.8`
- å…¶ä»–ä¾è³´ä¿æŒä¸è®Š

---

## æ–‡æª”çµæ§‹ / Documentation Structure

```
context-rot-cht/
â”œâ”€â”€ README.md (å·²æ›´æ–°)
â”œâ”€â”€ CHANGELOG.md (æ–°å¢)
â”œâ”€â”€ OLLAMA_INTEGRATION.md (æ–°å¢ - è‹±æ–‡)
â”œâ”€â”€ OLLAMA_INTEGRATION_zh-TW.md (æ–°å¢ - ä¸­æ–‡)
â”œâ”€â”€ MODIFICATIONS_SUMMARY.md (æœ¬æª”æ¡ˆ)
â”‚
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ README.md (å·²æ›´æ–°)
â”‚       â”œâ”€â”€ llm_judge.py (å·²ä¿®æ”¹)
â”‚       â””â”€â”€ providers/
â”‚           â”œâ”€â”€ __init__.py (æ–°å¢)
â”‚           â””â”€â”€ ollama.py (æ–°å¢)
â”‚
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ ollama_example.py (æ–°å¢)
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_ollama_provider.py (æ–°å¢)
â”‚
â””â”€â”€ requirements.txt (å·²æ›´æ–°)
```

---

## æ¨è–¦å·¥ä½œæµç¨‹ / Recommended Workflow

### 1. åˆæ¬¡è¨­ç½® / Initial Setup

```bash
# å®‰è£ Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# æ‹‰å–æ¨¡å‹
ollama pull llama3.1:8b
ollama pull qwen2.5:14b

# å®‰è£ Python ä¾è³´
pip install -r requirements.txt
```

### 2. é©—è­‰å®‰è£ / Verify Installation

```bash
# æª¢æŸ¥ Ollama
ollama list

# é‹è¡Œæ¸¬è©¦
python tests/test_ollama_provider.py
```

### 3. é–‹å§‹å¯¦é©— / Start Experimenting

```bash
# é‹è¡Œç¯„ä¾‹
python examples/ollama_example.py

# æˆ–åœ¨æ‚¨çš„å¯¦é©—ä¸­ä½¿ç”¨
# See experiments/models/README.md
```

---

## æ•ˆèƒ½è€ƒé‡ / Performance Considerations

### ç¡¬é«”éœ€æ±‚ / Hardware Requirements

| æ¨¡å‹å¤§å° | æœ€å°‘ RAM | æ¨è–¦é…ç½® |
|---------|---------|---------|
| 7B | 8GB | 16GB + GPU |
| 13-14B | 16GB | 32GB + GPU |
| 70B+ | 64GB+ | é«˜éš GPU |

### é€Ÿç‡é™åˆ¶ / Rate Limits

- âœ… æœ¬åœ°æ¨¡å‹ç„¡é€Ÿç‡é™åˆ¶
- âœ… å¯è¨­ç½® `max_tokens_per_minute=1000000`

---

## æœªä¾†æ”¹é€² / Future Enhancements

å¯èƒ½çš„æ”¹é€²æ–¹å‘ï¼š

- [ ] ä¸²æµå›æ‡‰æ”¯æ´
- [ ] å¤š GPU ä¸¦è¡Œè™•ç†
- [ ] è‡ªå‹•æ¨¡å‹æ‹‰å–
- [ ] è‡ªå‹•æª¢æ¸¬ä¸Šä¸‹æ–‡çª—å£
- [ ] é‡åŒ–é¸é …æ”¯æ´

---

## åƒè€ƒè³‡æº / References

- [Ollama å®˜æ–¹ç¶²ç«™](https://ollama.ai)
- [Ollama Python å‡½å¼åº«](https://github.com/ollama/ollama-python)
- [Context Rot è«–æ–‡](https://research.trychroma.com/context-rot)
- [åŸå§‹ Repository](https://github.com/chroma-core/context-rot)

---

## è²¢ç»è€… / Contributors

- åˆå§‹å¯¦ç¾ï¼šAcademic Experiment Fork (2025-12-02)

---

## ç‰ˆæœ¬è³‡è¨Š / Version Info

- **ç‰ˆæœ¬ / Version**: 1.0
- **æ—¥æœŸ / Date**: 2025-12-02
- **ç‹€æ…‹ / Status**: ç©©å®š / Stable
- **æ¸¬è©¦ç‹€æ…‹ / Test Status**: å·²é€šé / Passed

---

## æˆæ¬Š / License

èˆ‡åŸ Context Rot repository ä¿æŒç›¸åŒæˆæ¬Šã€‚

Same license as the original Context Rot repository.
